{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af462d05-c3f3-4d19-9d7d-630eaf8f8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from deepsurvloss import NegativeLogLikelihood, NegativeLogLikelihoodWithRegular\n",
    "from FusionE2EModel_crossattention import CrossMRIModel,ImageFusionModel,AttentionFusion\n",
    "from tqdm import tqdm\n",
    "from utils import c_index\n",
    "import random\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d8c714-3c84-4540-b859-2dbf1aafaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, neg_expend):\n",
    "        self.data_source = data_source\n",
    "        self.neg_expend = neg_expend\n",
    " \n",
    "    def __iter__(self):\n",
    "        indices_pos = [] \n",
    "        indices_neg = []\n",
    "        indices = []\n",
    "        for i in range(len(self.data_source)):\n",
    "            if self.data_source[i][1] == 1:\n",
    "                indices_pos.append(i)\n",
    "            else:\n",
    "                indices_neg.append(i)\n",
    "        # random.seed(1)\n",
    "        random.shuffle(indices_pos)\n",
    "        random.shuffle(indices_neg)\n",
    "        \n",
    "        for i in range(len(indices_pos)):\n",
    "            indices += [indices_pos[i], ] + indices_neg[i*self.neg_expend: (i+1)*self.neg_expend]\n",
    "        # print('pos_count:{}, neg_count:{}, indices:{}'.format(len(indices_pos), len(indices_neg), indices))\n",
    "        return iter(indices)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d1c1a9-53d5-487d-8924-f3cd9172c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_features = pd.read_csv('data/rad_features.csv')\n",
    "rad_features = rad_features.drop(columns=rad_features.columns[[1, 2]])\n",
    "rad_features.head\n",
    "clinical_features = pd.read_csv('data/Clifeatures.csv')\n",
    "clinical_features = clinical_features[['ID','VIRADS', 'Age', 'Recurrent tumor']]\n",
    "# clinical_features.head\n",
    "scaler = MinMaxScaler()\n",
    "clinical_features['Age'] = scaler.fit_transform(clinical_features[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf91d11-7876-4f23-8586-03b1e723b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_df = data_df\n",
    "        self.rad_features = rad_features\n",
    "        self.clif = clinical_features\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_df.iloc[idx]\n",
    "        pids = data['ID']\n",
    "        status = data['status']\n",
    "        times = data['time']\n",
    "        DWI_ImgTensor = torch.tensor(np.load(f'../npy_dwi_normed_224x224x12_ai/DWI{pids}.npy')).float()\n",
    "        T2_ImgTensor = torch.tensor(np.load(f'../npy_t2_normed_224x224x12_ai/T2{pids}.npy')).float()\n",
    "        rad_fe = self.rad_features[self.rad_features['ID'] == pids].drop('ID', axis=1)      \n",
    "        cli_fe = self.clif[self.clif['ID'] == pids].drop('ID', axis=1)\n",
    "        return pids, status, times, DWI_ImgTensor.cuda(), T2_ImgTensor.cuda(),torch.tensor(rad_fe.values).float().squeeze(0).cuda(), torch.tensor(cli_fe.values).float().squeeze(0).cuda()\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12df663-5f59-4d14-8f56-e67e34bb5278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fold_dir = f'resnet_model/final'\n",
    "os.makedirs(os.path.join(fold_dir, 'saved_models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(fold_dir, 'Figures'), exist_ok=True)\n",
    "all_df = pd.read_csv(f'data/label_group.csv')\n",
    "train_df = all_df[all_df['group'] == 'train'].reset_index(drop=True)\n",
    "val_df = all_df[all_df['group'] == 'val'].reset_index(drop=True)\n",
    "\n",
    "neg_expend = 3\n",
    "traindataset = MyDataset(train_df)\n",
    "trainloader = DataLoader(traindataset, batch_size=8, sampler=MySampler(traindataset, neg_expend=neg_expend))\n",
    "valdataset = MyDataset(val_df)\n",
    "valloader = DataLoader(valdataset, batch_size=8,shuffle=False)\n",
    "\n",
    "epochs = 100\n",
    "max_no_improve = 80  \n",
    "weight_decay = 0.05\n",
    "lr = 0.00001\n",
    "\n",
    "best_val_cindex = 0.0\n",
    "no_improve_count = 0\n",
    "train_cindex_list = []\n",
    "val_cindex_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = CrossMRIModel(device=device, img_encoder='resnet50').to(device)\n",
    "criterion = NegativeLogLikelihoodWithRegular('cuda', model, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_ids = []\n",
    "    train_status = []\n",
    "    train_times = []\n",
    "    train_outputs = []\n",
    "    all_loss_train = 0\n",
    "\n",
    "    for step, batch in enumerate(trainloader):\n",
    "        train_ids.extend(batch[0])\n",
    "        train_status.extend(batch[1])\n",
    "        train_times.extend(batch[2])\n",
    "        optimizer.zero_grad()\n",
    "        output, x = model(batch[3], batch[4], batch[5], batch[6])\n",
    "        output = output.squeeze(1)\n",
    "        train_outputs.extend(output.tolist())\n",
    "        loss = criterion(output.cuda(), batch[2].cuda(), batch[1].cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss_train += loss.item()  \n",
    "    \n",
    "\n",
    "    train_outputs = np.array(train_outputs)\n",
    "    train_ids = np.array(train_ids)\n",
    "    train_times = np.array(train_times)\n",
    "    train_status = np.array(train_status)\n",
    "    train_cindex = c_index(-train_outputs, train_times, train_status)\n",
    "    avg_train_loss = all_loss_train / len(trainloader)    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_ids = []\n",
    "        val_status = []\n",
    "        val_times = []\n",
    "        val_outputs = []\n",
    "        all_loss_val = 0\n",
    "        \n",
    "        for valbatch in valloader:\n",
    "            val_ids.extend(valbatch[0])\n",
    "            val_status.extend(valbatch[1])\n",
    "            val_times.extend(valbatch[2])\n",
    "            output, x = model(valbatch[3], valbatch[4], valbatch[5], valbatch[6])\n",
    "            output = output.squeeze(1)\n",
    "            val_outputs.extend(output.tolist())\n",
    "            loss = criterion(output.cuda(), valbatch[2].cuda(), valbatch[1].cuda())\n",
    "            all_loss_val += loss.item() \n",
    "        \n",
    "        val_outputs = np.array(val_outputs)\n",
    "        val_ids = np.array(val_ids)\n",
    "        val_times = np.array(val_times)\n",
    "        val_status = np.array(val_status)\n",
    "        val_cindex = c_index(-val_outputs, val_times, val_status)\n",
    "        avg_val_loss = all_loss_val / len(valloader)\n",
    "    \n",
    "    train_cindex_list.append(train_cindex)\n",
    "    val_cindex_list.append(val_cindex)\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}: '\n",
    "          f'train cindex:{train_cindex:.4f}, train loss:{avg_train_loss:.4f}; '\n",
    "          f'val cindex:{val_cindex:.4f}, val loss:{avg_val_loss:.4f}')\n",
    "\n",
    "\n",
    "    if val_cindex > best_val_cindex:\n",
    "        best_val_cindex = val_cindex\n",
    "        no_improve_count = 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "\n",
    "    if no_improve_count >= max_no_improve:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs with no improvement\")\n",
    "        save_path = os.path.join(fold_dir, 'saved_models', 'final_model.pth')\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Saved final model (early stopped) at: {save_path}\")\n",
    "        break  \n",
    "\n",
    "if no_improve_count < max_no_improve:\n",
    "    save_path = os.path.join(fold_dir, 'saved_models', 'final_model.pth')\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Saved final model (completed all epochs) at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31f410-efda-49f8-918d-6b1a50f80888",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models_dir = os.path.join(fold_dir, 'saved_models')\n",
    "final_model_path = os.path.join(saved_models_dir, 'final_model.pth')\n",
    "\n",
    "if os.path.exists(final_model_path):\n",
    "    model.load_state_dict(torch.load(final_model_path))\n",
    "    print(f\"final_model.pth\")\n",
    "\n",
    "def predict_risk(data_loader, group_name):\n",
    "    model.eval()\n",
    "    all_ids = []\n",
    "    all_risks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            ids = batch[0]\n",
    "            outputs, _ = model(batch[3], batch[4], batch[5], batch[6])\n",
    "            risks = outputs.squeeze(1).cpu().numpy()\n",
    "            \n",
    "            all_ids.extend(ids)\n",
    "            all_risks.extend(risks)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'ID': all_ids,\n",
    "        'risk': all_risks,\n",
    "        'group': group_name\n",
    "    })\n",
    "\n",
    "    merged = results.merge(all_df[['ID', 'status', 'time']], on='ID', how='left')\n",
    "    return merged[['ID', 'status', 'time', 'risk', 'group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2c7a9-fee3-46aa-bbf7-68ffe2c0289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = MyDataset(train_df)\n",
    "trainloader = DataLoader(traindataset, batch_size=32,shuffle=False)\n",
    "multi_df = all_df[all_df['group'] == 'multi'].reset_index(drop=True)\n",
    "multidataset = MyDataset(multi_df)\n",
    "multiloader = DataLoader(multidataset, batch_size=32, shuffle=False)\n",
    "nac_df = all_df[all_df['group'] == 'nac'].reset_index(drop=True)\n",
    "nacdataset = MyDataset(nac_df)\n",
    "nacloader = DataLoader(nacdataset, batch_size=32, shuffle=False)\n",
    "\n",
    "train_results = predict_risk(trainloader, 'train')\n",
    "val_results = predict_risk(valloader, 'val')\n",
    "multi_results = predict_risk(multiloader, 'multi')\n",
    "nac_results = predict_risk(nacloader, 'nac')\n",
    "\n",
    "all_results = pd.concat([train_results, val_results, multi_results, nac_results], ignore_index=True)\n",
    "output_csv = os.path.join(fold_dir, f'risk_predictions.csv')\n",
    "all_results.to_csv(output_csv, index=False)\n",
    "train_cindex = c_index(-train_results['risk'].values, train_results['time'].values, train_results['status'].values)\n",
    "val_cindex = c_index(-val_results['risk'].values, val_results['time'].values, val_results['status'].values)\n",
    "multi_cindex = c_index(-multi_results['risk'].values, multi_results['time'].values, multi_results['status'].values)\n",
    "nac_cindex = c_index(-nac_results['risk'].values, nac_results['time'].values, nac_results['status'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
